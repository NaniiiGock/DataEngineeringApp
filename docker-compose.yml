version: '3.8'

services:

  db:
    image: postgres:13
    container_name: postgres_db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: mydatabase
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - app_network

  streamlit:
    build:
      context: ./UI
    container_name: streamlit_app
    depends_on:
      - db
    environment:
      DB_HOST: db
      DB_PORT: 5432
      DB_NAME: mydatabase
      DB_USER: postgres
      DB_PASSWORD: password
    ports:
      - "8501:8501"
    networks:
      - app_network

  nginx:
    image: nginx:latest
    container_name: nginx_proxy
    depends_on:
      - streamlit
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./certs:/etc/nginx/certs
    networks:
      - app_network

  airflow:
    build:
      context: ./etl
      dockerfile: Dockerfile
    container_name: airflow_service
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@postgres/airflow'
    volumes:
      - ./etl/airflow/dags:/opt/airflow/dags
      - ./etl/airflow/logs:/opt/airflow/logs
      - ./etl/airflow/plugins:/opt/airflow/plugins
      - ./etl/my_dbt_project:/dbt
      - ./etl/airflow/data:/app/data 
      # - ./etl/iceberg:/app/etl/iceberg
      - /var/run/docker.sock:/var/run/docker.sock 
      
    ports:
      - "8082:8080"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - app_network



  duckdb:
    image: datacatering/duckdb:v1.1.1
    platform: linux/amd64
    volumes:
      - ./etl/airflow/data:/data
    stdin_open: true
    tty: true
    networks:
      - app_network

  dbt:
    build:
      context: ./etl/my_dbt_project
      dockerfile: Dockerfile
    container_name: dbt_service
    stdin_open: true
    tty: true
    environment:
      DBT_PROFILES_DIR: "/dbt"  
    volumes:
      - ./etl/my_dbt_project:/dbt  
      - ./etl/airflow/data:/app/data
    working_dir: /dbt          
    platform: linux/amd64      
    command: ["tail", "-f", "/dev/null"] 
    networks:
      - app_network

  analytics:
    container_name: analytics
    build:
      context: service_api/
    ports:
      - 5001:5001
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
    volumes:
      - ./service_api:/serving
    networks:
      - app_network

  spark:
    image: bitnami/spark:3.5.0
    container_name: spark_service
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
    volumes:
      - ./etl:/app/etl
      - ./etl/airflow/data:/app/data 
      - ./etl/iceberg/warehouse:/app/etl/iceberg/warehouse 
      - ./etl/iceberg/jars:/opt/spark/jars 
    entrypoint: ["/opt/bitnami/scripts/spark/entrypoint.sh"]
    command: ["bash", "-c", "tail -f /dev/null"]
    networks:
      - app_network
    depends_on:
      - airflow
      - duckdb

networks:
  app_network:
    driver: bridge

volumes:
  db_data:
  postgres_data:
