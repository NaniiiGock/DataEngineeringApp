version: '3.8'

services:

  db:
    image: postgres:13
    container_name: postgres_db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: password
      POSTGRES_DB: mydatabase
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - app_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - app_network

  streamlit:
    build:
      context: ./UI
    container_name: streamlit_app
    depends_on:
      - db
    environment:
      DB_HOST: db
      DB_PORT: 5432
      DB_NAME: mydatabase
      DB_USER: postgres
      DB_PASSWORD: password
    ports:
      - "8501:8501"
    networks:
      - app_network

  jupyter:
    image: jupyter/minimal-notebook
    volumes:
      - ./etl/airflow/data:/data
    ports:
      - "8888:8888"
    command: start-notebook.sh --NotebookApp.token=''
    networks:
      - app_network

  nginx:
    image: nginx:latest
    container_name: nginx_proxy
    depends_on:
      - streamlit
    ports:
      - "443:443"
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./certs:/etc/nginx/certs
    networks:
      - app_network

  airflow:
    build:
      context: ./etl
      dockerfile: Dockerfile
    container_name: airflow_service
    environment:
      KAGGLE_USERNAME: 'gustavnikopensiusut'
      KAGGLE_KEY: "d33d24666d0d887067a73795a679cbf5"
      PIP_ADDITIONAL_REQUIREMENTS: 'kaggle pandas duckdb'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: 'admin'
      _AIRFLOW_WWW_USER_PASSWORD: 'sergilolidt천eliseltsuuredk채ed'
      _AIRFLOW_WWW_USER_FIRSTNAME: 'Gustav'
      _AIRFLOW_WWW_USER_LASTNAME: 'Nikopensius'
      _AIRFLOW_WWW_USER_EMAIL: 'gustav.nikopensius@ut.ee'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__EXECUTOR: 'LocalExecutor'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@postgres/airflow'
    volumes:
      - ./etl/airflow/dags:/opt/airflow/dags
      - ./etl/airflow/logs:/opt/airflow/logs
      - ./etl/airflow/plugins:/opt/airflow/plugins
      - ./etl/airflow/data:/opt/airflow/data
      - ./etl/wait-for-postgres1.sh:/wait-for-postgres.sh

    ports:
      - "8082:8080"
    depends_on:
      - postgres
    networks:
      - app_network
    command: >
      bash -c "./wait-for-postgres1.sh postgres airflow db init &&
      airflow users create --username admin --password sergilolidt천eliseltsuuredk채ed --firstname Gustav --lastname Nikopensius --role Admin --email gustav.nikopensius@ut.ee &&
      airflow scheduler & airflow webserver"


  duckdb:
    image: datacatering/duckdb:v1.1.1
    platform: linux/amd64
    volumes:
      - ./etl/airflow/data:/data
    stdin_open: true
    tty: true
    networks:
      - app_network

  dbt:
    build:
      context: ./etl/my_dbt_project
      dockerfile: Dockerfile
    container_name: dbt_service
    stdin_open: true
    tty: true
    environment:
      DBT_PROFILES_DIR: "/dbt"  
    volumes:
      - ./etl/my_dbt_project:/dbt  
    working_dir: /dbt          
    platform: linux/amd64      
    command: ["dbt", "debug"]  

  analytics:
    container_name: analytics
    build:
      context: service_api/
    ports:
      - 5001:5001
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
    volumes:
      - ./service_api:/serving
    networks:
      - app_network

networks:
  app_network:
    driver: bridge

volumes:
  db_data:
  postgres_data:
