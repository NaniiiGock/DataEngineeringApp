services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow:
    image: my_airflow_image
    environment:
      KAGGLE_USERNAME: 'gustavnikopensiusut'
      KAGGLE_KEY: "d33d24666d0d887067a73795a679cbf5"
      PIP_ADDITIONAL_REQUIREMENTS: 'kaggle pandas duckdb'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: 'admin'
      _AIRFLOW_WWW_USER_PASSWORD: 'sergilolidtõeliseltsuuredkäed'
      _AIRFLOW_WWW_USER_FIRSTNAME: 'Gustav'
      _AIRFLOW_WWW_USER_LASTNAME: 'Nikopensius'
      _AIRFLOW_WWW_USER_EMAIL: 'gustav.nikopensius@ut.ee'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__EXECUTOR: 'LocalExecutor'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'postgresql+psycopg2://airflow:airflow@postgres/airflow'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
    ports:
      - "8082:8080"
    depends_on:
      - postgres
    command: >
      bash -c "airflow db init && airflow scheduler & airflow webserver"

  dbt:
    image: ghcr.io/dbt-labs/dbt-core:1.8.8
    environment:
      DBT_PROFILES_DIR: "/dbt"
    volumes:
      - ./dbt:/dbt  # Mount dbt project folder
      - ./data:/data  # Folder to store data pulled from APIs, etc.
    working_dir: /dbt

  duckdb:
    image: datacatering/duckdb:v1.1.1
    volumes:
      - ./data:/data  # Share a volume with dbt to store DuckDB files here

  # Jupyter service environment for testing data processing
  jupyter:
    image: jupyter/minimal-notebook
    volumes:
      - ./data:/data  # Share data volume with other services
    ports:
      - "8888:8888"
    command: start-notebook.sh --NotebookApp.token=''


volumes:
  postgres_data: